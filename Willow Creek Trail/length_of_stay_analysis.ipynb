{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5391b63b",
   "metadata": {},
   "source": [
    "## CV Parking Lot Analysis\n",
    "\n",
    "#### Date: 9/30/2025\n",
    "#### Author: Nineveh O'Connell\n",
    "\n",
    "Goal: This notebook ingests csv output of computer vision model of the willow creek trailhead and output length of stay/turnover metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53aced58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41ca23",
   "metadata": {},
   "source": [
    "Define parking spot outlines (spot_id, minx, maxy, miny) and sort by minx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac058ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spot_boundaries = pd.DataFrame({\n",
    "    \"spot_id\": np.arange(1, 17),\n",
    "    \"minx\": [640, 549, 490, 430, 380, 330, 280, 240, 195, 165, 140, 112, 102, 75, 40, 5],\n",
    "    \"maxy\": [340, 340, 340, 340, 335, 325, 320, 320, 330, 320, 315, 310, 305, 300, 295, 295],\n",
    "    \"miny\": [265] * 16\n",
    "}).sort_values(\"minx\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f8fc9",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be52ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ts = \"2025-10-01 20-39-35\"\n",
    "\n",
    "# 2) read in data (adjust path as needed)\n",
    "csv_path = \"/Users/Nineveh.OConnell/OneDrive - DOT OST/volpe-portfolio-PublicLands - AI Real-Time Parking Project/Data/data_download_export_\" + file_ts + \".csv\"\n",
    "in_willow_creek = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea33777",
   "metadata": {},
   "source": [
    "Assign to parking spots, calling the parking spot -1 in the case of being in the roadway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "459a29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_willow_creek['parking_spot_id'] = pd.cut(in_willow_creek['cx'], bins = spot_boundaries['minx'], labels = np.arange(1,16)[::-1], right = True)\n",
    "\n",
    "# if midpoint has y higher (below) than 335, mark it as being in the roadway\n",
    "in_willow_creek['parking_spot_id'] = in_willow_creek['parking_spot_id'].cat.add_categories('-1')\n",
    "in_willow_creek.loc[in_willow_creek['cy'] > 335, 'parking_spot_id'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89821fe4",
   "metadata": {},
   "source": [
    "Make confidence into a numeric variable and only keep instances with confidence over 0.35. From spot checking, instances with lower confidence are not really vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67bf2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) extract numeric confidence from a string like \"label (0.82)\" into confidence_numeric\n",
    "#    regex captures the number inside parentheses (first occurrence)\n",
    "def extract_confidence(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    m = re.search(r\"\\(([^)]+)\\)\", str(s))\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "in_willow_creek[\"confidence_numeric\"] = in_willow_creek[\"confidence\"].apply(extract_confidence)\n",
    "\n",
    "# keep only rows where confidence is 0.35 or greater\n",
    "willow_creek_vehicles = in_willow_creek[in_willow_creek[\"confidence_numeric\"] > 0.35]\n",
    "# Clean up column\n",
    "willow_creek_vehicles = willow_creek_vehicles.drop(columns=[\"confidence\"])\n",
    "# make timestamp actual date time object, and turn id into a string\n",
    "willow_creek_vehicles['timestamp_dt'] = pd.to_datetime(willow_creek_vehicles['timestamp'], format='%Y-%m-%d %H-%M-%S')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbc5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "312e1880",
   "metadata": {},
   "source": [
    "In cleaning up data resulting from computer vision of vehicles in the parking lot, I have noticed that sometimes the computer vision model re-recognized a vehicle after too many frames of the id missing, it wasn’t feasibly the same vehicle. In the future we can toggle this in the tracker itself by setting a track_buffer threshold. For now, the code below pulls out the unique and comprehensive list of timestamps within, and adds a unique suffix to future records of an id if the same id appears again in the dataset after missing from the data for 5 or more frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c2b9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_gaps_and_suffix_ids(df,\n",
    "                                   id_col=\"id\",\n",
    "                                   ts_col=\"timestamp\",\n",
    "                                   gap_threshold=25,\n",
    "                                   inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    # *** NEW: Drop IDs which appear only once in the unique timestamps ***\n",
    "    id_counts = df.groupby(id_col)[ts_col].nunique()\n",
    "    valid_ids = id_counts[id_counts > 1].index\n",
    "    df = df[df[id_col].isin(valid_ids)].copy()\n",
    "\n",
    "    # Build sorted unique timestamps and mapping timestamp -> frame index\n",
    "    unique_ts = pd.Index(sorted(df[ts_col].unique()))\n",
    "    ts_to_index = {ts: idx for idx, ts in enumerate(unique_ts)}\n",
    "\n",
    "    df[\"_frame_index\"] = df[ts_col].map(ts_to_index)\n",
    "    df = df.sort_values([id_col, \"_frame_index\"]).reset_index(drop=True)\n",
    "\n",
    "    new_ids = []\n",
    "    state = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        orig_id = row[id_col]\n",
    "        fi = int(row[\"_frame_index\"])\n",
    "\n",
    "        if orig_id not in state:\n",
    "            state[orig_id] = {\"last_idx\": fi, \"counter\": 0}\n",
    "            new_ids.append(orig_id)\n",
    "            continue\n",
    "\n",
    "        last_idx = state[orig_id][\"last_idx\"]\n",
    "        gap = fi - last_idx\n",
    "\n",
    "        if gap >= gap_threshold:\n",
    "            state[orig_id][\"counter\"] += 1\n",
    "            suffix = state[orig_id][\"counter\"]\n",
    "            new_id = f\"{orig_id}_{suffix}\"\n",
    "            state[orig_id][\"last_idx\"] = fi\n",
    "            new_ids.append(new_id)\n",
    "        else:\n",
    "            cnt = state[orig_id][\"counter\"]\n",
    "            if cnt == 0:\n",
    "                new_ids.append(orig_id)\n",
    "            else:\n",
    "                new_ids.append(f\"{orig_id}_{cnt}\")\n",
    "            state[orig_id][\"last_idx\"] = fi\n",
    "\n",
    "    df[\"id_suffixed\"] = new_ids\n",
    "    df = df.sort_index()\n",
    "    df = df.drop(columns=[\"_frame_index\"])\n",
    "\n",
    "    return df, unique_ts\n",
    "\n",
    "# use above function\n",
    "if __name__ == \"__main__\":\n",
    "    willow_creek_vehicles, unique_ts = split_long_gaps_and_suffix_ids(willow_creek_vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9de6bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nineveh.OConnell\\AppData\\Local\\Temp\\ipykernel_18324\\2009858674.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for (ts_val, spot_val), group in willow_creek_vehicles.groupby(group_cols, sort=False):\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "tolerance = 2                # pixels; consider duplicates when |dx| <= tolerance and |dy| <= tolerance\n",
    "\n",
    "# will collect kept rows\n",
    "kept_indices = []\n",
    "\n",
    "# group by timestamp and parking_spot_id\n",
    "group_cols = [\"timestamp\", \"parking_spot_id\"]\n",
    "\n",
    "# iterate groups — this is efficient unless you have extremely many tiny groups\n",
    "for (ts_val, spot_val), group in willow_creek_vehicles.groupby(group_cols, sort=False):\n",
    "\n",
    "    # convert group to numpy arrays for speed\n",
    "    # sort by confidence desc so first kept are the best\n",
    "    order = np.argsort(-group[\"confidence_numeric\"])\n",
    "    group_idx = group.index.to_numpy()[order]\n",
    "    cx_arr = group.loc[group_idx, \"cx\"].to_numpy()\n",
    "    cy_arr = group.loc[group_idx, \"cy\"].to_numpy()\n",
    "\n",
    "    # keep list of indices for this group\n",
    "    kept_for_group = []\n",
    "\n",
    "    # iterate detections in descending confidence order\n",
    "    for i, idx in enumerate(group_idx):\n",
    "        cx_i = cx_arr[i]\n",
    "        cy_i = cy_arr[i]\n",
    "\n",
    "        # if cx or cy is NaN, treat as not matching any kept point (so it can be kept only if best)\n",
    "        if np.isnan(cx_i) or np.isnan(cy_i):\n",
    "            # if the best entry is NaN and there are others non-NaN, this NaN will still be kept only\n",
    "            # if it has the highest confidence; follow the same logic as R code where exact matches matter\n",
    "            # Here, proceed to keep if no kept point exists (or if it's highest confidence)\n",
    "            if len(kept_for_group) == 0:\n",
    "                kept_for_group.append(idx)\n",
    "            continue\n",
    "\n",
    "        # check against already-kept points: if any kept point is within tolerance in both x and y, skip\n",
    "        if kept_for_group:\n",
    "            kept_cx = willow_creek_vehicles.loc[kept_for_group, \"cx\"].to_numpy()\n",
    "            kept_cy = willow_creek_vehicles.loc[kept_for_group, \"cy\"].to_numpy()\n",
    "\n",
    "            # compute boolean mask of kept points within tolerance (|dx| <= tol and |dy| <= tol)\n",
    "            # using broadcasting\n",
    "            dx = np.abs(kept_cx - cx_i)\n",
    "            dy = np.abs(kept_cy - cy_i)\n",
    "            within_tol = (dx <= tolerance) & (dy <= tolerance)\n",
    "\n",
    "            if np.any(within_tol):\n",
    "                # a kept point already within tolerance — consider current row a duplicate -> skip\n",
    "                continue\n",
    "\n",
    "        # otherwise, keep this detection\n",
    "        kept_for_group.append(idx)\n",
    "\n",
    "    # extend global kept list\n",
    "    kept_indices.extend(kept_for_group)\n",
    "\n",
    "# create deduplicated DataFrame preserving original relative order of kept rows\n",
    "kept_mask = willow_creek_vehicles.index.isin(kept_indices)\n",
    "willow_creek_vehicles = willow_creek_vehicles.loc[kept_mask].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf25982",
   "metadata": {},
   "source": [
    "Ok, now I feel good about the general confidence level and deduplicating we've applied. There may be more errors to catch, but this way hopefully a single vehicle should be recognized for longer as being in the same place. By vehicle id and class and parking spot, let's capture the earliest time it was seen and last time it was seen. Perhaps let's also capture the lowest confidence and highest confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fc7ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create concatenated string uniquely identifying vehicles to make my life easier\n",
    "willow_creek_vehicles['id_string'] = willow_creek_vehicles['id_suffixed'].astype(str)\n",
    "willow_creek_vehicles['parking_spot_id_str'] = willow_creek_vehicles['parking_spot_id'].astype(str)\n",
    "\n",
    "# output min and max time stamps for each vehicle\n",
    "vehicle_time_boundaries = willow_creek_vehicles.groupby('id_string')['timestamp_dt'].agg(['min', 'max', 'count'])\n",
    "vehicle_confidence_boundaries = willow_creek_vehicles.groupby('id_string')['confidence_numeric'].agg(['min', 'max', 'median'])\n",
    "vehicle_parking_spot_list = willow_creek_vehicles.groupby('id_string')['parking_spot_id_str'].agg(lambda x: ','.join(x.unique().astype(str)))\n",
    "\n",
    "# merge together\n",
    "unique_vehicles_summary = pd.merge(vehicle_time_boundaries, vehicle_confidence_boundaries, on = 'id_string', suffixes= (\"_time\", \"_conf\"))\n",
    "unique_vehicles_summary = pd.merge(willow_creek_vehicles[['id_string']].drop_duplicates(), unique_vehicles_summary, on = 'id_string')\n",
    "unique_vehicles_summary = pd.merge(unique_vehicles_summary, vehicle_parking_spot_list, on = 'id_string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "704297d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate length of stay\n",
    "unique_vehicles_summary['length_of_stay_minutes'] = (unique_vehicles_summary['max_time'] - unique_vehicles_summary['min_time']).dt.total_seconds()/60\n",
    "\n",
    "# remove additional entries with overall low confidence\n",
    "unique_vehicles_summary = unique_vehicles_summary[(unique_vehicles_summary['max_conf'] > 0.6) & (unique_vehicles_summary['median'] > 0.5)]\n",
    "\n",
    "# and remove entries with less than two minutes of recognition -- I don't trust the cv like that, this only happened when I was pulling the same frame multiple times before\n",
    "unique_vehicles_summary = unique_vehicles_summary[(unique_vehicles_summary['length_of_stay_minutes'] > 1.5)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b97b3f",
   "metadata": {},
   "source": [
    "Additional error resolution is needed, as vehicles may be identified incorrectly multiple times. If one parking spot has multiple vehicles recognized as being present with overlapping time bands, keep only the record with the wider timeband. This should also work by identifying the record with the most counts, but you'd have to identify the time band anyway to know which to remove -- so let's apply this more rigorous approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a09b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_widest_nonoverlapping(df, spot_col='parking_spot_id_str', t0_col='min_time', t1_col='max_time'):\n",
    "\n",
    "    kept_indices = []\n",
    "\n",
    "    # process per spot\n",
    "    for spot, group in df.groupby(spot_col, sort=False):\n",
    "        # sort by duration desc, stable so ties preserve original order\n",
    "        group_sorted = group.sort_values('length_of_stay_minutes', ascending=False, kind='stable')\n",
    "\n",
    "        kept_for_spot = []\n",
    "        kept_intervals = []  # list of (min_time, max_time) for kept intervals\n",
    "\n",
    "        for idx, row in group_sorted.iterrows():\n",
    "            t0 = row[t0_col]\n",
    "            t1 = row[t1_col]\n",
    "            # check overlap with any already-kept interval\n",
    "            # overlap condition: not (t1 <= kept_t0 or t0 >= kept_t1)\n",
    "            overlaps = any((t1 > kt0) and (t0 < kt1) for (kt0, kt1) in kept_intervals)\n",
    "            if not overlaps:\n",
    "                kept_for_spot.append(idx)\n",
    "                kept_intervals.append((t0, t1))\n",
    "\n",
    "        kept_indices.extend(kept_for_spot)\n",
    "\n",
    "    # return filtered dataframe\n",
    "    result = df.loc[kept_indices]\n",
    "    return result\n",
    "\n",
    "# deduplicate nested time spans\n",
    "unique_vehicles_summary_dedup = keep_widest_nonoverlapping(unique_vehicles_summary,\n",
    "                                                         spot_col='parking_spot_id_str',\n",
    "                                                         t0_col='min_time',\n",
    "                                                         t1_col='max_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834c231",
   "metadata": {},
   "source": [
    "Ok, great. We end up with a similar number of vehicles to my manual count -- this indicates 14 vehicles (seeing as one is just the truck that was passing by) and I manually counted 15 vehicles over this time period. Vehicles should be unique by id -- the class can vary -- and the parking spot identification is tricky and not requisite to deem a vehicle as uniquely identified, though it was useful for deduplication because I specifically looked at nesting time frames (overlapping, non-nested time frames should be permitted). Length of stay should ideally exclude vehicles that were there at the beginning and those that were still there at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67f4d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_start_time = unique_vehicles_summary_dedup['min_time'].agg('min')\n",
    "cv_end_time = unique_vehicles_summary_dedup['max_time'].agg('max')\n",
    "\n",
    "# flag as present start of period if so, end of period do the same. allow two minute buffer because sometimes a vehilce is mixed in a given frame\n",
    "unique_vehicles_summary_dedup['flag_present_start'] = (unique_vehicles_summary_dedup['min_time'] - timedelta(minutes=2)) < cv_start_time\n",
    "unique_vehicles_summary_dedup['flag_present_end']   = (unique_vehicles_summary_dedup['max_time'] + timedelta(minutes=2)) > cv_end_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4f9bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this operation, drop vehicles that were not in a parking spot\n",
    "parked_cars_only = unique_vehicles_summary_dedup[unique_vehicles_summary_dedup['parking_spot_id_str'] != '-1']\n",
    "\n",
    "# summarize length of stay by unique car, ignoring spot now\n",
    "parked_cars_only = parked_cars_only.groupby('id_string')[['length_of_stay_minutes', 'flag_present_start', 'flag_present_end']].agg('sum')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94bea7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between  2025-10-01 17:49:46  and  2025-10-01 20:39:35 ,  17  total vehicles were captured at the Willow Creek Trailhead.\n",
      " 7  were already parked when this time period began and  9  were present when this time period ended. \n",
      " We witnessed the beginning and end of the visit to the trailhead for  3  vehicles. The average stay for these vehicles was  48.36111111111111  minutes.\n"
     ]
    }
   ],
   "source": [
    "# count overall unique vehicles in the dataset\n",
    "n_parked_in_period = len(parked_cars_only)\n",
    "# count unique vehicles present at the start\n",
    "n_parked_at_start = sum(parked_cars_only['flag_present_start'] == 1)\n",
    "# count unique vehicles present at the end\n",
    "n_parked_at_end = sum(parked_cars_only['flag_present_end'] == 1)\n",
    "# count unique vehicles for which we witnessed the beginning and end\n",
    "n_whole_stay_witnessed = len(parked_cars_only[(parked_cars_only['flag_present_end'] == 0) & (parked_cars_only['flag_present_start'] == 0)])\n",
    "# average length of stay\n",
    "avg_length_of_stay = parked_cars_only[(parked_cars_only['flag_present_end'] == 0) & (parked_cars_only['flag_present_start'] == 0)]['length_of_stay_minutes'].agg('mean')\n",
    "# print all those things\n",
    "print(\"Between \", cv_start_time, \" and \", cv_end_time, \", \", n_parked_in_period, \" total vehicles were captured at the Willow Creek Trailhead.\\n\", \n",
    "      n_parked_at_start, \" were already parked when this time period began and \", n_parked_at_end, \" were present when this time period ended. \\n \" \\\n",
    "      \"We witnessed the beginning and end of the visit to the trailhead for \",\n",
    "      n_whole_stay_witnessed, \" vehicles. The average stay for these vehicles was \", avg_length_of_stay, \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c624e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique vehicle info in post processing folder\n",
    "unique_vehicles_summary_dedup.to_csv('/Users/Nineveh.OConnell/OneDrive - DOT OST/volpe-portfolio-PublicLands - AI Real-Time Parking Project/Postprocessing/willow_creek_parking_summary_' + file_ts + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Public-Lands-Computer-Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
